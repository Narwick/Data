{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_eonWE4nHoN"
      },
      "source": [
        "Esse é um projeto em que estou fazendo para aprimorar minhas Skills de Data Cleaning.<br>\n",
        "O Dataset escolhido foi: Full anime list (20k+) in MAL 2023.<br>\n",
        "Está disponível no Kaggle nesse link: https://www.kaggle.com/datasets/crxxom/all-animes-in-mal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx_SyF1jnHoO"
      },
      "source": [
        "O Dataset será limpado através dos seguintes passos: <br>\n",
        "1. Checar o número de colunas<br>\n",
        "2. Alterar os nomes das colunas para os nomes corretos.<br>\n",
        "3. Checar valores vazios/nulos.<br>\n",
        "4. Checar valores duplicados<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TcNaUuygnHoO"
      },
      "outputs": [],
      "source": [
        "# O primeiro passo é importar as Libs que serão utilizadas\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NUIq9FanHoP",
        "outputId": "6d5f7bc0-ca20-42f8-e1dc-15af9c68c2a3"
      },
      "outputs": [],
      "source": [
        "# Agora importaremos o Dataset\n",
        "df = pd.read_csv(\"mal_anime.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eEtMqTInHoP",
        "outputId": "da2fec61-56fd-460d-93e8-105811a1a324"
      },
      "outputs": [],
      "source": [
        "# Vamos checar todas as colunas\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A6BihLM2nHoP"
      },
      "outputs": [],
      "source": [
        "# Vamos apagar a coluna Unnamed pois não faz sentido tê-la no momento.\n",
        "df.drop(columns=['Unnamed: 0'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UstLHhJ6nHoP"
      },
      "outputs": [],
      "source": [
        "# Agora iremos renomear as colunas restantes com um nome mais conciso.\n",
        "df.rename(columns={\n",
        "                'title':'Title',\n",
        "                'episodes':'Episode Count',\n",
        "                'status':'Airing Status',\n",
        "                'theme':'Theme',\n",
        "                'demographic':'Demographic Target',\n",
        "                'genres':'Genres',\n",
        "                'type':'Type (e.g., TV Show, Movie)',\n",
        "                'favorites':'Favorites Count',\n",
        "                'popularity':'Popularity Rank',\n",
        "                'rank':'Overall Rank',\n",
        "                'score':'Average User Score',\n",
        "                'members':'Members Count',\n",
        "                'synopsis':'Synopsis',\n",
        "                'aired':'Airing Date',\n",
        "                'duration':'Duration per Episode',\n",
        "                'premiered':'Premiered Season',\n",
        "                'studios':'Production Studio'},inplace=True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3iLPphRnHoQ",
        "outputId": "e02bd436-b3c7-4f50-cd70-f240b175e5b9"
      },
      "outputs": [],
      "source": [
        "# Vamos checar\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hk_LxWPqnHoQ",
        "outputId": "31816efd-8651-4da3-df56-6e6dd8c78be8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Agora vamos checar se a registros duplicados\n",
        "df.duplicated().sum()\n",
        "# Com isso confirmamos que não existem registros duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaEjyc_0nHoQ",
        "outputId": "b0266a68-932b-495a-dc1d-495efed65a7f"
      },
      "outputs": [],
      "source": [
        "# Vamos checar as colunas que possuem valores nulos no Dataset\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZjN0mCynHoQ",
        "outputId": "8ff5e588-2ee0-43dd-ff61-a2fe521264a9"
      },
      "outputs": [],
      "source": [
        "# Podemos ver que possuimos muitos nulos nas colunas \"Overall Rank\" e \"Average User Score\", então vamos trata-las.\n",
        "# Primeiro vamos verificar o tipo do objeto dessas colunas\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GAR4QbYqnHoR"
      },
      "outputs": [],
      "source": [
        "# Vamos retirar a \"#\" da coluna \"Overall Rank\" e preencher os valores nulos como 0.\n",
        "df['Overall Rank'] = df['Overall Rank'].str.replace('#','')\n",
        "df['Overall Rank'].fillna('0',inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sduSV0Q4nHoR"
      },
      "outputs": [],
      "source": [
        "# Vamos agora converter para inteiro.\n",
        "df['Overall Rank'] = df['Overall Rank'].astype('Int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UBYTJkMnnHoR"
      },
      "outputs": [],
      "source": [
        "# Agora vamos tratar a coluna \"Popularity Rank\".\n",
        "df['Popularity Rank'] = df['Popularity Rank'].str.replace('#','')\n",
        "df['Popularity Rank'] = df['Popularity Rank'].astype('int32')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "a9Wpk3j1nHoR"
      },
      "outputs": [],
      "source": [
        "# Agora vamos tratar a coluna \"Average User Score\".\n",
        "df['Average User Score'].fillna(0.00,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EilGtPUYnHoR",
        "outputId": "cdb66fb5-335c-40f8-e00b-879109b2aec4"
      },
      "outputs": [],
      "source": [
        "# Vamos checar se está tudo certo agora.\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWPu0bVPnHoR",
        "outputId": "89fc95fb-03fe-4d95-faad-e1e327385fb8"
      },
      "outputs": [],
      "source": [
        "# Seguindo essa linha vamos converter as outras colunas para o tipo correto.\n",
        "# Vamos começar pela coluna \"Episode Count\"\n",
        "df['Episode Count'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ODMPQT88nHoR"
      },
      "outputs": [],
      "source": [
        "# Como no caso dessa coluna ela representa o número de episodios já lançados, a algumas séries que ainda estão em lançamento então para representar essa séries inacabadas optei por usar o -1 para representa-las\n",
        "df['Episode Count'] = df['Episode Count'].str.replace('Unknown','-1')\n",
        "df['Episode Count'] = df['Episode Count'].astype(\"int32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YJ25NzgQnHoR"
      },
      "outputs": [],
      "source": [
        "# Vamos seguir para próxima coluna\n",
        "df['Favorites Count'] = df['Favorites Count'].str.replace(',','')\n",
        "df['Favorites Count'] = df['Favorites Count'].astype(\"int32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0LrXxXzCnHoS"
      },
      "outputs": [],
      "source": [
        "# Próxima\n",
        "df['Members Count'] = df['Members Count'].str.replace(',','')\n",
        "df['Members Count'] = df['Members Count'].astype(\"int32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RYDgTSDBnHoS"
      },
      "outputs": [],
      "source": [
        "# Estudando esse Dataset resolvi fazer algumas modificações\n",
        "# A Primeira é transformar em duas colunas a coluna \"Airing Date\"\n",
        "# Uma de Data início de transmissão e outra de encerramento\n",
        "new = df['Airing Date'].str.split('to',n=1,expand=True)\n",
        "df['Start of Transmission'] = new[0]\n",
        "df['End of Transmission'] = new[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yem21BTXnHoS"
      },
      "outputs": [],
      "source": [
        "# Vamos dropar a coluna que usamos\n",
        "df.drop(columns=['Airing Date'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-uUH_kUPnHoS"
      },
      "outputs": [],
      "source": [
        "# Agora faremos o mesmo com a Coluna \"Premiered Season\"\n",
        "new = df['Premiered Season'].str.split(' ',n=1,expand=True)\n",
        "df['Premiered Season'] = new[0]\n",
        "df['Premiered Year'] = new[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Por fim vamos exportar o Dataset agora que já foi realizado o processo de Data Cleaning\n",
        "df.to_csv(\"C:/Users/João Victor/Desktop/Python/Data/my_anime_list_project/data_analysis/mal_cleaned.csv\",sep=\";\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zadke1tUnHoS"
      },
      "source": [
        "Com isso encerro o processo de Data Cleaning deste Dataset, a seguir farei um novo notebook para trabalhar as skills de Data Analysis."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
